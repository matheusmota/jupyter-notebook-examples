{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apache Spark with Scala\n",
    "\n",
    "More details and magic commands here: https://github.com/apache/incubator-toree/blob/master/etc/examples/notebooks/magic-tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count problem\n",
    "\n",
    "Defining a object for counting frequence of words from 2 files and printing the common words that appear more than 100 times on both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "\n",
    "\n",
    "class Analiser {\n",
    "\n",
    "  // Args = path/to/text0.txt path/to/text1.txt\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    // create Spark context with Spark configuration\n",
    "    //não precisa, já que está em modo shell\n",
    "    //val sc = new SparkContext(new SparkConf().setAppName(\"Contagem de Palavra\"))\n",
    "    //val ssc = new StreamingContext(sc, Seconds(1))\n",
    "\n",
    "    val startTime = System.nanoTime()    \n",
    "      \n",
    "    println(\"TEXT1\")\n",
    "\n",
    "    // read first text file and split into lines\n",
    "    val txt1 = sc.textFile(args(0))\n",
    "\n",
    "    // contar palavras do texto 1 e imprimir as 5 palavras com as maiores ocorrencias (ordem DECRESCENTE)\n",
    "\n",
    "    val wordsTxt1 = txt1.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    val wordCountTxt1 = wordsTxt1.map(word => (word.toLowerCase.replaceAll(\"[,.!?:;]\",\"\"), 1))\n",
    "            .reduceByKey(_ + _)\n",
    "            .filter(item => item._1.length > 3) // apenas palavras com mais de tres caracteres\n",
    "            .map(item => item.swap) // esse trecho é necessário pois não há função sortByValue() para values, então inverteremos\n",
    "            .sortByKey(false)\n",
    "            .map(item => item.swap)\n",
    "\n",
    "    val output1 = wordCountTxt1.take(5) // apenas as 5 mais frequentes\n",
    "\n",
    "    output1.foreach(item => println(item._1 + \"=\" + item._2.toString))\n",
    "\n",
    "    println(\"TEXT2\")\n",
    "\n",
    "    // read second text file and split each document into words\n",
    "    val txt2 = sc.textFile(args(1))\n",
    "\n",
    "    // transformações do texto 2 análogas ao texto 1\n",
    "\n",
    "    val wordsTxt2 = txt2.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    val wordCountTxt2 = wordsTxt2.map(word => (word.toLowerCase.replaceAll(\"[,.!?:;]\",\"\"),1))\n",
    "            .reduceByKey(_ + _) // list1(k1, v1) list2(k2,v2) -> if(k1.value==k2.value): merge k1 with k2 in (k3, v1+v2)\n",
    "            .filter(item => item._1.length > 3) // apenas palavras com mais de tres caracteres\n",
    "            .map(item => item.swap) // esse trecho é necessário pois não há função sortByValue() para values, então inverteremos\n",
    "            .sortByKey(false)\n",
    "            .map(item => item.swap)\n",
    "\n",
    "    val output2 = wordCountTxt2.take(5)\n",
    "\n",
    "    output2.foreach(item => println(item._1 + \"=\" + item._2.toString))\n",
    "\n",
    "    // comparar resultado e imprimir na ordem ALFABETICA todas as palavras que aparecem MAIS que 100 vezes nos 2 textos\n",
    "\n",
    "    println(\"COMMON\")\n",
    "\n",
    "    val filt1 = wordCountTxt1.filter(_._2 > 100).sortByKey().keys // palavras do texto 1\n",
    "    val filt2 = wordCountTxt2.filter(_._2 > 100).sortByKey().keys // palavras do texto 2\n",
    "      \n",
    "    val common =  filt1.intersection(filt2) // intersecção\n",
    "\n",
    "    common.collect().toList.sorted.foreach{ x => println(x)}\n",
    "      \n",
    "     val endTime = System.nanoTime()\n",
    "     println(\"Time elapsed: \" + (endTime-startTime)/100000000 + \" seconds\")\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's run the object ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1\n",
      "said=456\n",
      "alice=377\n",
      "that=234\n",
      "with=172\n",
      "very=139\n",
      "TEXT2\n",
      "that=759\n",
      "with=448\n",
      "were=365\n",
      "from=326\n",
      "they=302\n",
      "COMMON\n",
      "little\n",
      "said\n",
      "that\n",
      "they\n",
      "this\n",
      "with\n",
      "Time elapsed: 11 seconds\n"
     ]
    }
   ],
   "source": [
    "val files = Array(\"./resources/data/input1.txt\", \"./resources/data/input2.txt\")\n",
    "\n",
    "val myAnaliser = new Analiser\n",
    "\n",
    "myAnaliser.main(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Desired Output:  \n",
    "TEXT1  \n",
    "said=456  \n",
    "alice=377  \n",
    "that=234  \n",
    "with=172  \n",
    "very=139  \n",
    "TEXT2  \n",
    "that=759  \n",
    "with=448  \n",
    "were=365  \n",
    "from=326  \n",
    "they=302  \n",
    "COMMON  \n",
    "little  \n",
    "said  \n",
    "that  \n",
    "they  \n",
    "this  \n",
    "with  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

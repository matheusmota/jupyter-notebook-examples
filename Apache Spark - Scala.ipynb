{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apache Spark with Scala\n",
    "\n",
    "More details and magic commands here: https://github.com/apache/incubator-toree/blob/master/etc/examples/notebooks/magic-tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "\n",
    "\n",
    "object Analisador {\n",
    "\n",
    "  // Args = path/to/text0.txt path/to/text1.txt\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    // create Spark context with Spark configuration\n",
    "    //não precisa, já que está em modo shell\n",
    "    //val sc = new SparkContext(new SparkConf().setAppName(\"Contagem de Palavra\"))\n",
    "    //val ssc = new StreamingContext(sc, Seconds(1))\n",
    "      \n",
    "    println(\"TEXT1\")\n",
    "\n",
    "    // read first text file and split into lines\n",
    "    val lines1 = sc.textFile(args(0))\n",
    "\n",
    "    // contar palavras do texto 1 e imprimir as 5 palavras com as maiores ocorrencias (ordem DECRESCENTE)\n",
    "\n",
    "    val words = lines1.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    val data = words.map(word => (word.replaceAll(\"[,.!?:;]\",\"\"), 1))\n",
    "            .reduceByKey(_ + _)\n",
    "            .filter(_._1.length > 3) // apenas palavras com mais de tres caracteres\n",
    "            .map(_.swap) // esse trecho é necessário pois não há função sortByValue()\n",
    "            .sortByKey(false,1)\n",
    "            .map(_.swap)\n",
    "\n",
    "    val output = data.take(5) // apenas as 5 mais frequentes\n",
    "\n",
    "    output.foreach{ x => println(x._1 + \"=\" + x._2.toString)} // imprimir em cada linha: \"palavra=numero\"\n",
    "\n",
    "    println(\"TEXT2\")\n",
    "\n",
    "    // read second text file and split each document into words\n",
    "    val lines2 = sc.textFile(args(1))\n",
    "\n",
    "    // transformações do texto 2 análogas ao texto 1\n",
    "\n",
    "    val words2 = lines2.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    val data2 = words2.map(word => (word.replaceAll(\"[,.!?:;]\",\"\"),1))\n",
    "            .reduceByKey(_ + _)\n",
    "            .filter(_._1.length > 3)\n",
    "            .map(_.swap)\n",
    "            .sortByKey(false,1)\n",
    "            .map(_.swap)\n",
    "\n",
    "    val output2 = data2.take(5)\n",
    "\n",
    "    output2.foreach{ x => println(x._1 + \"=\" + x._2.toString)}\n",
    "\n",
    "    // comparar resultado e imprimir na ordem ALFABETICA todas as palavras que aparecem MAIS que 100 vezes nos 2 textos\n",
    "\n",
    "    println(\"COMMON\")\n",
    "\n",
    "    val filt1 = data.filter(_._2 > 100).sortByKey().keys // palavras do texto 1\n",
    "\n",
    "    val filt2 = data2.filter(_._2 > 100).sortByKey().keys // palavras do texto 2\n",
    "\n",
    "    val data3 =  filt1.intersection(filt2) // intersecção\n",
    "\n",
    "    val output3 = data3.map(word => (word,1)).sortByKey() // rdd expandido pois não consegui ordenar um rdd com array de apenas uma string\n",
    "\n",
    "    output3.foreach{ x => println(x._1)} // imprimir em cada linha: \"palavra\"\n",
    "\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1\n",
      "said=451\n",
      "Alice=377\n",
      "that=224\n",
      "with=170\n",
      "very=126\n",
      "TEXT2\n",
      "that=733\n",
      "with=435\n",
      "were=363\n",
      "from=312\n",
      "they=225\n",
      "COMMON\n"
     ]
    }
   ],
   "source": [
    "val files = Array(\"./resources/data/input1.txt\", \"./resources/data/input2.txt\")\n",
    "\n",
    "Analisador.main(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desired output:\n",
    "TEXT1\n",
    "said=456\n",
    "alice=377\n",
    "that=234\n",
    "with=172\n",
    "very=139\n",
    "TEXT2\n",
    "that=759\n",
    "with=448\n",
    "were=365\n",
    "from=326\n",
    "they=302\n",
    "COMMON\n",
    "little\n",
    "said\n",
    "that\n",
    "they\n",
    "this\n",
    "with\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
